
![272a59d5dcb24116851519938d34d140-1766412196462](https://github.com/user-attachments/assets/8997fa69-bd85-448a-b5c7-f93f33b9c249)

## LangGraph Chatbot (OpenRouter + Streamlit)

Full‑stack demo that streams AI chat replies over WebSockets, keeps multi‑turn memory in Postgres via LangGraph checkpoints, and ships with Docker Compose for one‑command local runs.

### Features
- WebSocket streaming UI built with Streamlit; token-by-token rendering with graceful errors.
- LangGraph backend using `ChatOpenAI` against OpenRouter; configurable model via `MODEL_NAME`.
- Conversation memory persisted in Postgres through `langgraph-checkpoint-postgres` for resumable sessions.
- Session management in the UI (new/switch/delete chats with autogenerated titles).
- Health endpoint (`/health`) and structured logging for backend readiness checks.

### Architecture
- `frontend/streamlit_app.py`: Streamlit app that opens `WEBSOCKET_URL/{session_id}`, streams tokens, and manages local session list/state.
- `backend/chatbot_langgraph.py`: FastAPI + LangGraph workflow; streams `on_chat_model_stream` events from OpenRouter to the client.
- `docker-compose.yml`: Services for Postgres, backend, and frontend with sensible defaults and healthchecks.
- `backend/` & `frontend/` Dockerfiles: Python 3.11 slim images, minimal deps.

### Prerequisites
- Docker + Docker Compose (recommended) or Python 3.11 with virtualenv.
- OpenRouter API key.

### Quickstart (Docker)
1) Create an `.env` next to `docker-compose.yml`:
```
OPENROUTER_API_KEY=sk-...
MODEL_NAME=tngtech/deepseek-r1t2-chimera:free
```
2) Start the stack:
```
docker compose up --build
```
3) Visit Streamlit at http://localhost:8501 (backend at http://localhost:8000).

### Manual Run (no Docker)
Backend:
```
cd backend
python -m venv .venv && . .venv/bin/activate
pip install -r requirements.txt
export DATABASE_URL=postgresql://user:password@localhost:5432/chatbot_db
export OPENROUTER_API_KEY=sk-...
export MODEL_NAME=tngtech/deepseek-r1t2-chimera:free
uvicorn chatbot_langgraph:app --reload
```
Frontend (new shell):
```
cd frontend
python -m venv .venv && . .venv/bin/activate
pip install -r requirements.txt
export WEBSOCKET_URL=ws://localhost:8000/ws/chat
streamlit run streamlit_app.py
```

### How it works
- Client sends JSON `{"message": "..."} ` over WebSocket per session ID.
- Backend LangGraph uses `AsyncPostgresSaver` to checkpoint conversation state keyed by `thread_id=session_id`.
- Graph streams `on_chat_model_stream` chunks; backend forwards tokens as `{type:"token", content:"..."}` and signals completion with `{type:"complete"}`.
- Frontend appends tokens live and stores messages in `st.session_state.sessions`, including timestamps and user-friendly titles.

### Configuration
- `OPENROUTER_API_KEY` (required): OpenRouter key.
- `MODEL_NAME` (required): Model slug, e.g., `tngtech/deepseek-r1t2-chimera:free`.
- `DATABASE_URL` (backend): Postgres connection string.
- `WEBSOCKET_URL` (frontend): WebSocket endpoint base.

### Useful commands
- Run backend directly: `uvicorn chatbot_langgraph:app --host 0.0.0.0 --port 8000`
- Run frontend directly: `streamlit run streamlit_app.py`
- DB health (compose): `docker compose exec db pg_isready -U user`

### Notes for CV/portfolio
- Demonstrates streaming AI UX, conversation memory with database-backed checkpoints, and containerized deployment.
- Showcases LangGraph orchestration with OpenRouter models and FastAPI WebSockets.
- Includes typed Pydantic DTOs, logging, health checks, and minimal infra IaC via Docker Compose.

